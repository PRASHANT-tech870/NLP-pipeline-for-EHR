{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14c4d9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFIRST OBTAIN CHARACTER EMBEDDINGS USING BIDIRECTIONAL LSTM\\n\\nTHEN, CONCAT THE CHARACTER EMBEDDINGS TO OBTAIN WORD EMBEDDINGS\\n\\nPASS THIS THROUGH A BiLSTM ENCODER\\n\\nTHEN CONCAT THE EMBEDDINGS: Vobt + Vpre_trained = Vword\\n\\nPASS THESE Vword through an LSTM and then through a softmax function for performing NER\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "FIRST OBTAIN CHARACTER EMBEDDINGS USING BIDIRECTIONAL LSTM\n",
    "\n",
    "THEN, CONCAT THE CHARACTER EMBEDDINGS TO OBTAIN WORD EMBEDDINGS\n",
    "\n",
    "PASS THIS THROUGH A BiLSTM ENCODER\n",
    "\n",
    "THEN CONCAT THE EMBEDDINGS: Vobt + Vpre_trained = Vword\n",
    "\n",
    "PASS THESE Vword through an LSTM and then through a softmax function for performing NER\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85dfe2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sentence.', 'And this is another sentence?', 'Finally, this is the last sentence.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return sentences\n",
    "\n",
    "# Example usage:\n",
    "text = \"This is a sentence. And this is another sentence? Finally, this is the last sentence.\"\n",
    "sentences = split_into_sentences(text)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb100239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"zhengyun21/PMC-Patients\")\n",
    "\n",
    "l = []\n",
    "for i in range(len(dataset['train'])):\n",
    "    if ('brain' in dataset['train'][i]['patient'] and 'MRI' in dataset['train'][i]['patient']):\n",
    "        l.append(i)\n",
    "\n",
    "#TO EXTRACT HISTORY OF THE FIRST PATIENT IN THE LIST, USE:\n",
    "dataset['train'][11]\n",
    "\n",
    "osl = []\n",
    "for i in range(5):\n",
    "    osl.append(dataset['train'][l[i]]['patient'])\n",
    "\n",
    "# piped_out = []\n",
    "# for j in range(50):\n",
    "#     original_string = osl[j]\n",
    "#     piped_out.append(pipe(original_string))\n",
    "\n",
    "# osl[2]\n",
    "\n",
    "#LET US OBTAIN DATA ONLY UPTO MRI\n",
    "l = []\n",
    "for i in range(len(osl)):\n",
    "    t = split_into_sentences(osl[i])\n",
    "    s = \"\"\n",
    "    for j in range(len(t)):\n",
    "        s += t[j]\n",
    "        if 'MRI' in t[j]:\n",
    "            break\n",
    "    l.append(s)\n",
    " \n",
    "    osl[i] = s\n",
    "    \n",
    "#OSL IS THE ACTUAL DATA INSIDE THE LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "84e40d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"zhengyun21/PMC-Patients\")\n",
    "\n",
    "# l = []\n",
    "# for i in range(len(dataset['train'])):\n",
    "#     if ('brain' in dataset['train'][i]['patient'] and 'MRI' in dataset['train'][i]['patient']):\n",
    "#         l.append(i)\n",
    "\n",
    "# tsl = []\n",
    "# for i in range(1800,2000):\n",
    "#     tsl.append(dataset['train'][l[i]]['patient'])\n",
    "\n",
    "# l = []\n",
    "# for i in range(len(tsl)):\n",
    "#     t = split_into_sentences(tsl[i])\n",
    "#     s = \"\"\n",
    "#     for j in range(len(t)):\n",
    "#         s += t[j]\n",
    "#         if 'MRI' in t[j]:\n",
    "#             break\n",
    "#     l.append(s)\n",
    "#     tsl[i] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824c0680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import string\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"max\") # pass device=0 if using gpu\n",
    "labelled_data = []\n",
    "\n",
    "\n",
    "for m in range(len(osl)):\n",
    "\n",
    "    c = 0\n",
    "    x = 0\n",
    "    o = 0\n",
    "    l1 = []\n",
    "    for i in range(len(osl[m].split('.'))):\n",
    "        t = (pipe(osl[m].split('.')[i]))\n",
    "        for j in range(len(t)):\n",
    "            if t[j]['entity_group'] != 'Age' and  t[j]['entity_group'] != 'Sex' and t[j]['entity_group'] != 'Sign_symptom' and t[j]['entity_group'] != 'Duration' and t[j]['entity_group'] != 'Biological_structure':\n",
    "                c+=1\n",
    "        # print(t)\n",
    "        # print(c)\n",
    "        if c == len(t):\n",
    "            o = i\n",
    "            \n",
    "            x += 1\n",
    "        else:\n",
    "            l1.append(osl[m].split('.')[i])\n",
    "        c = 0\n",
    "    # if  print(pipe(osl[509].split('.')[i])) == []:\n",
    "    \n",
    "    # print(osl[m].split('.')[o])\n",
    "    # print(x)\n",
    "    # print(len(l1))\n",
    "    length = 0\n",
    "    s = \"\"\n",
    "    for i in range(len(l1)):\n",
    "        length += len(l1[i].split())\n",
    "        s += l1[i]\n",
    "        s += \". \"\n",
    "    osl[m] = s\n",
    "    # print(length)\n",
    "    # print(s)\n",
    "\n",
    "\n",
    "# for m in range(len(tsl)):\n",
    "\n",
    "#     c = 0\n",
    "#     x = 0\n",
    "#     o = 0\n",
    "#     l1 = []\n",
    "#     for i in range(len(tsl[m].split('.'))):\n",
    "#         t = (pipe(tsl[m].split('.')[i]))\n",
    "#         for j in range(len(t)):\n",
    "#             if t[j]['entity_group'] != 'Age' and  t[j]['entity_group'] != 'Sex' and t[j]['entity_group'] != 'Sign_symptom' and t[j]['entity_group'] != 'Duration' and t[j]['entity_group'] != 'Biological_structure':\n",
    "#                 c+=1\n",
    "#         # print(t)\n",
    "#         # print(c)\n",
    "#         if c == len(t):\n",
    "#             o = i\n",
    "            \n",
    "#             x += 1\n",
    "#         else:\n",
    "#             l1.append(tsl[m].split('.')[i])\n",
    "#         c = 0\n",
    "#     # if  print(pipe(osl[509].split('.')[i])) == []:\n",
    "    \n",
    "#     # print(osl[m].split('.')[o])\n",
    "#     # print(x)\n",
    "#     # print(len(l1))\n",
    "#     length = 0\n",
    "#     s = \"\"\n",
    "#     for i in range(len(l1)):\n",
    "#         length += len(l1[i].split())\n",
    "#         s += l1[i]\n",
    "#         s += \". \"\n",
    "#     tsl[m] = s\n",
    "    # print(length)\n",
    "    # print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "76804ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(osl) - 1, -1, -1):\n",
    "    if len(osl[i].split()) > 1000:\n",
    "        osl.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "2784d6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(osl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2abf222b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', '24-year-old', 'healthy', 'woman', 'presented', 'with', 'difficulty', 'breathing', 'and', 'dissatisfaction', 'with', 'her', 'facial', 'appearance', 'she', 'had', 'a', 'history', 'of', 'childhood', 'trauma', 'resulting', 'in', 'nasal', 'septum', 'deviation', 'and', 'external', 'nasal', 'deformity', 'four', 'months', 'after', 'a', 'successful', 'and', 'uneventful', 'septorhinoplasty', 'she', 'presented', 'to', 'the', 'emergency', 'department', 'with', 'blunt', 'nasal', 'trauma', 'resulting', 'in', 'a', 'septal', 'hematoma', 'which', 'was', 'drained', 'successfully', 'the', 'patient', 'was', 'discharged', 'with', 'no', 'adverse', 'sequelae', 'nfour', 'months', 'later', 'the', 'patient', 'sustained', 'nasal', 'trauma', 'again', 'this', 'time', 'accompanied', 'by', 'clear', 'nasal', 'discharge', 'raising', 'suspicion', 'of', 'cerebrospinal', 'fluid', 'csf', 'leak', 'the', 'patient', 'was', 'discharged', 'after', 'managing', 'the', 'nasal', 'injury', 'as', 'the', 'ct', 'brain', 'showed', 'an', 'intact', 'cribriform', 'plate', 'with', 'no', 'evidence', 'of', 'a', 'csf', 'leak', 'ten', 'days', 'later', 'she', 'presented', 'at', 'the', 'emergency', 'department', 'with', 'dizziness', 'and', 'an', 'unstable', 'gait', 'she', 'also', 'had', 'complaints', 'of', 'paresthesia', 'for', 'the', 'past', 'two', 'months', 'beginning', 'in', 'her', 'right', 'hand', 'and', 'progressing', 'to', 'the', 'right', 'shoulder', 'arm', 'and', 'leg', 'associated', 'with', 'some', 'difficulty', 'in', 'the', 'execution', 'of', 'movements', 'in', 'the', 'first', 'and', 'second', 'finger', 'of', 'the', 'right', 'hand', 'her', 'right', 'leg', 'was', 'quite', 'stiff', 'with', 'difficulty', 'in', 'walking', 'on', 'close', 'inquiry', 'she', 'gave', 'history', 'of', 'pain', 'in', 'the', 'right', 'eye', 'and', 'double', 'vision', 'many', 'months', 'back', 'which', 'had', 'resolved', 'spontaneously', 'nmagnetic', 'resonance', 'imaging', 'mri', 'of', 'the', 'brain', 'cervical', 'and', 'thoracic', 'spine', 'demonstrated', 'demyelinating', 'lesions', 'in', 'the', 'brain', 'and', 'cervical', 'segment', 'of', 'the', 'spinal', 'cord', 'figure']\n",
      "['a', '59-year-old', 'female', 'current', 'smoker', 'with', '20', 'pack-years', 'history', 'with', 'a', 'past', 'medical', 'history', 'only', 'significant', 'for', 'hypertension', 'gradually', 'developed', 'anorexia', 'nausea', 'fatigue', 'and', 'weight', 'loss', 'she', 'initially', 'presented', 'to', 'the', 'emergency', 'department', 'with', 'left', 'flank', 'pain', 'and', 'on', 'ct', 'scan', 'of', 'the', 'abdomen', 'was', 'found', 'to', 'have', 'diffuse', 'osteosclerotic', 'lesions', 'in', 'visualized', 'bones', 'there', 'was', 'no', 'palpable', 'mass', 'or', 'axillary', 'adenopathy', 'on', 'breast', 'examination', 'she', 'had', 'multiple', 'mammograms', 'in', 'the', 'past', 'some', 'of', 'which', 'had', 'shown', 'suspicious', 'architecture', 'which', 'was', 'followed', 'up', 'with', 'multiple', 'breast', 'ultrasounds', 'that', 'had', 'revealed', 'benign', 'findings', 'nuclear', 'bone', 'scan', 'was', 'unremarkable', 'ct', 'chest', 'revealed', 'no', 'pulmonary', 'lesions', 'but', 'there', 'were', 'small', 'mediastinal', 'submental', 'and', 'axillary', 'lymphadenopathy', 'and', 'several', 'subcutaneous', 'lesions', 'on', 'the', 'back', 'one', 'of', 'which', 'was', 'excised', 'and', 'showed', 'inclusion', 'epidermal', 'cyst', 'nwhile', 'the', 'workup', 'was', 'ongoing', 'the', 'patient', 'started', 'to', 'experience', 'lower', 'back', 'pain', 'associated', 'with', 'weakness', 'of', 'lower', 'extremities', 'numbness', 'tingling', 'and', 'balance', 'issues', 'she', 'developed', 'constipation', 'as', 'well', 'as', 'urinary', 'incontinence', 'mri', 'of', 'the', 'brain', 'and', 'spine', 'redemonstrated', 'similar', 'bony', 'lesions', 'in', 'vertebrae', 'and', 'also', 'revealed', 'abnormal', 'leptomeningeal', 'enhancement', 'in', 'the', 'brainstem', 'extending', 'along', 'the', 'entire', 'spinal', 'cord', 'figure']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    # Tokenize the sentence and remove special characters\n",
    "    return re.findall(r'\\b[\\w-]+\\b', sentence.lower())\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    tokenized_sentences = []\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize each sentence\n",
    "        tokens = tokenize_sentence(sentence)\n",
    "        tokenized_sentences.append(tokens)\n",
    "    return tokenized_sentences\n",
    "# Tokenize the sentences\n",
    "osl_tokenized = tokenize_sentences(osl)\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    # Tokenize the sentence and remove special characters\n",
    "    return re.findall(r'\\b[\\w-]+\\b', sentence.lower())\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenize each sentence\n",
    "        tokens = tokenize_sentence(sentence)\n",
    "        tokenized_sentences.append(tokens)\n",
    "    return tokenized_sentences\n",
    "# Tokenize the sentences\n",
    "\n",
    "print(osl_tokenized[0])\n",
    "print(osl_tokenized[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c3c895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a',\n",
       "  '24-year-old',\n",
       "  'healthy',\n",
       "  'woman',\n",
       "  'presented',\n",
       "  'with',\n",
       "  'difficulty',\n",
       "  'breathing',\n",
       "  'and',\n",
       "  'dissatisfaction',\n",
       "  'with',\n",
       "  'her',\n",
       "  'facial',\n",
       "  'appearance',\n",
       "  'she',\n",
       "  'had',\n",
       "  'a',\n",
       "  'history',\n",
       "  'of',\n",
       "  'childhood',\n",
       "  'trauma',\n",
       "  'resulting',\n",
       "  'in',\n",
       "  'nasal',\n",
       "  'septum',\n",
       "  'deviation',\n",
       "  'and',\n",
       "  'external',\n",
       "  'nasal',\n",
       "  'deformity',\n",
       "  'four',\n",
       "  'months',\n",
       "  'after',\n",
       "  'a',\n",
       "  'successful',\n",
       "  'and',\n",
       "  'uneventful',\n",
       "  'septorhinoplasty',\n",
       "  'she',\n",
       "  'presented',\n",
       "  'to',\n",
       "  'the',\n",
       "  'emergency',\n",
       "  'department',\n",
       "  'with',\n",
       "  'blunt',\n",
       "  'nasal',\n",
       "  'trauma',\n",
       "  'resulting',\n",
       "  'in',\n",
       "  'a',\n",
       "  'septal',\n",
       "  'hematoma',\n",
       "  'which',\n",
       "  'was',\n",
       "  'drained',\n",
       "  'successfully',\n",
       "  'the',\n",
       "  'patient',\n",
       "  'was',\n",
       "  'discharged',\n",
       "  'with',\n",
       "  'no',\n",
       "  'adverse',\n",
       "  'sequelae',\n",
       "  'nfour',\n",
       "  'months',\n",
       "  'later',\n",
       "  'the',\n",
       "  'patient',\n",
       "  'sustained',\n",
       "  'nasal',\n",
       "  'trauma',\n",
       "  'again',\n",
       "  'this',\n",
       "  'time',\n",
       "  'accompanied',\n",
       "  'by',\n",
       "  'clear',\n",
       "  'nasal',\n",
       "  'discharge',\n",
       "  'raising',\n",
       "  'suspicion',\n",
       "  'of',\n",
       "  'cerebrospinal',\n",
       "  'fluid',\n",
       "  'csf',\n",
       "  'leak',\n",
       "  'the',\n",
       "  'patient',\n",
       "  'was',\n",
       "  'discharged',\n",
       "  'after',\n",
       "  'managing',\n",
       "  'the',\n",
       "  'nasal',\n",
       "  'injury',\n",
       "  'as',\n",
       "  'the',\n",
       "  'ct',\n",
       "  'brain',\n",
       "  'showed',\n",
       "  'an',\n",
       "  'intact',\n",
       "  'cribriform',\n",
       "  'plate',\n",
       "  'with',\n",
       "  'no',\n",
       "  'evidence',\n",
       "  'of',\n",
       "  'a',\n",
       "  'csf',\n",
       "  'leak',\n",
       "  'ten',\n",
       "  'days',\n",
       "  'later',\n",
       "  'she',\n",
       "  'presented',\n",
       "  'at',\n",
       "  'the',\n",
       "  'emergency',\n",
       "  'department',\n",
       "  'with',\n",
       "  'dizziness',\n",
       "  'and',\n",
       "  'an',\n",
       "  'unstable',\n",
       "  'gait',\n",
       "  'she',\n",
       "  'also',\n",
       "  'had',\n",
       "  'complaints',\n",
       "  'of',\n",
       "  'paresthesia',\n",
       "  'for',\n",
       "  'the',\n",
       "  'past',\n",
       "  'two',\n",
       "  'months',\n",
       "  'beginning',\n",
       "  'in',\n",
       "  'her',\n",
       "  'right',\n",
       "  'hand',\n",
       "  'and',\n",
       "  'progressing',\n",
       "  'to',\n",
       "  'the',\n",
       "  'right',\n",
       "  'shoulder',\n",
       "  'arm',\n",
       "  'and',\n",
       "  'leg',\n",
       "  'associated',\n",
       "  'with',\n",
       "  'some',\n",
       "  'difficulty',\n",
       "  'in',\n",
       "  'the',\n",
       "  'execution',\n",
       "  'of',\n",
       "  'movements',\n",
       "  'in',\n",
       "  'the',\n",
       "  'first',\n",
       "  'and',\n",
       "  'second',\n",
       "  'finger',\n",
       "  'of',\n",
       "  'the',\n",
       "  'right',\n",
       "  'hand',\n",
       "  'her',\n",
       "  'right',\n",
       "  'leg',\n",
       "  'was',\n",
       "  'quite',\n",
       "  'stiff',\n",
       "  'with',\n",
       "  'difficulty',\n",
       "  'in',\n",
       "  'walking',\n",
       "  'on',\n",
       "  'close',\n",
       "  'inquiry',\n",
       "  'she',\n",
       "  'gave',\n",
       "  'history',\n",
       "  'of',\n",
       "  'pain',\n",
       "  'in',\n",
       "  'the',\n",
       "  'right',\n",
       "  'eye',\n",
       "  'and',\n",
       "  'double',\n",
       "  'vision',\n",
       "  'many',\n",
       "  'months',\n",
       "  'back',\n",
       "  'which',\n",
       "  'had',\n",
       "  'resolved',\n",
       "  'spontaneously',\n",
       "  'nmagnetic',\n",
       "  'resonance',\n",
       "  'imaging',\n",
       "  'mri',\n",
       "  'of',\n",
       "  'the',\n",
       "  'brain',\n",
       "  'cervical',\n",
       "  'and',\n",
       "  'thoracic',\n",
       "  'spine',\n",
       "  'demonstrated',\n",
       "  'demyelinating',\n",
       "  'lesions',\n",
       "  'in',\n",
       "  'the',\n",
       "  'brain',\n",
       "  'and',\n",
       "  'cervical',\n",
       "  'segment',\n",
       "  'of',\n",
       "  'the',\n",
       "  'spinal',\n",
       "  'cord',\n",
       "  'figure'],\n",
       " ['a',\n",
       "  '59-year-old',\n",
       "  'female',\n",
       "  'current',\n",
       "  'smoker',\n",
       "  'with',\n",
       "  '20',\n",
       "  'pack-years',\n",
       "  'history',\n",
       "  'with',\n",
       "  'a',\n",
       "  'past',\n",
       "  'medical',\n",
       "  'history',\n",
       "  'only',\n",
       "  'significant',\n",
       "  'for',\n",
       "  'hypertension',\n",
       "  'gradually',\n",
       "  'developed',\n",
       "  'anorexia',\n",
       "  'nausea',\n",
       "  'fatigue',\n",
       "  'and',\n",
       "  'weight',\n",
       "  'loss',\n",
       "  'she',\n",
       "  'initially',\n",
       "  'presented',\n",
       "  'to',\n",
       "  'the',\n",
       "  'emergency',\n",
       "  'department',\n",
       "  'with',\n",
       "  'left',\n",
       "  'flank',\n",
       "  'pain',\n",
       "  'and',\n",
       "  'on',\n",
       "  'ct',\n",
       "  'scan',\n",
       "  'of',\n",
       "  'the',\n",
       "  'abdomen',\n",
       "  'was',\n",
       "  'found',\n",
       "  'to',\n",
       "  'have',\n",
       "  'diffuse',\n",
       "  'osteosclerotic',\n",
       "  'lesions',\n",
       "  'in',\n",
       "  'visualized',\n",
       "  'bones',\n",
       "  'there',\n",
       "  'was',\n",
       "  'no',\n",
       "  'palpable',\n",
       "  'mass',\n",
       "  'or',\n",
       "  'axillary',\n",
       "  'adenopathy',\n",
       "  'on',\n",
       "  'breast',\n",
       "  'examination',\n",
       "  'she',\n",
       "  'had',\n",
       "  'multiple',\n",
       "  'mammograms',\n",
       "  'in',\n",
       "  'the',\n",
       "  'past',\n",
       "  'some',\n",
       "  'of',\n",
       "  'which',\n",
       "  'had',\n",
       "  'shown',\n",
       "  'suspicious',\n",
       "  'architecture',\n",
       "  'which',\n",
       "  'was',\n",
       "  'followed',\n",
       "  'up',\n",
       "  'with',\n",
       "  'multiple',\n",
       "  'breast',\n",
       "  'ultrasounds',\n",
       "  'that',\n",
       "  'had',\n",
       "  'revealed',\n",
       "  'benign',\n",
       "  'findings',\n",
       "  'nuclear',\n",
       "  'bone',\n",
       "  'scan',\n",
       "  'was',\n",
       "  'unremarkable',\n",
       "  'ct',\n",
       "  'chest',\n",
       "  'revealed',\n",
       "  'no',\n",
       "  'pulmonary',\n",
       "  'lesions',\n",
       "  'but',\n",
       "  'there',\n",
       "  'were',\n",
       "  'small',\n",
       "  'mediastinal',\n",
       "  'submental',\n",
       "  'and',\n",
       "  'axillary',\n",
       "  'lymphadenopathy',\n",
       "  'and',\n",
       "  'several',\n",
       "  'subcutaneous',\n",
       "  'lesions',\n",
       "  'on',\n",
       "  'the',\n",
       "  'back',\n",
       "  'one',\n",
       "  'of',\n",
       "  'which',\n",
       "  'was',\n",
       "  'excised',\n",
       "  'and',\n",
       "  'showed',\n",
       "  'inclusion',\n",
       "  'epidermal',\n",
       "  'cyst',\n",
       "  'nwhile',\n",
       "  'the',\n",
       "  'workup',\n",
       "  'was',\n",
       "  'ongoing',\n",
       "  'the',\n",
       "  'patient',\n",
       "  'started',\n",
       "  'to',\n",
       "  'experience',\n",
       "  'lower',\n",
       "  'back',\n",
       "  'pain',\n",
       "  'associated',\n",
       "  'with',\n",
       "  'weakness',\n",
       "  'of',\n",
       "  'lower',\n",
       "  'extremities',\n",
       "  'numbness',\n",
       "  'tingling',\n",
       "  'and',\n",
       "  'balance',\n",
       "  'issues',\n",
       "  'she',\n",
       "  'developed',\n",
       "  'constipation',\n",
       "  'as',\n",
       "  'well',\n",
       "  'as',\n",
       "  'urinary',\n",
       "  'incontinence',\n",
       "  'mri',\n",
       "  'of',\n",
       "  'the',\n",
       "  'brain',\n",
       "  'and',\n",
       "  'spine',\n",
       "  'redemonstrated',\n",
       "  'similar',\n",
       "  'bony',\n",
       "  'lesions',\n",
       "  'in',\n",
       "  'vertebrae',\n",
       "  'and',\n",
       "  'also',\n",
       "  'revealed',\n",
       "  'abnormal',\n",
       "  'leptomeningeal',\n",
       "  'enhancement',\n",
       "  'in',\n",
       "  'the',\n",
       "  'brainstem',\n",
       "  'extending',\n",
       "  'along',\n",
       "  'the',\n",
       "  'entire',\n",
       "  'spinal',\n",
       "  'cord',\n",
       "  'figure'],\n",
       " ['a',\n",
       "  '33-year-old',\n",
       "  'female',\n",
       "  'with',\n",
       "  'no',\n",
       "  'prior',\n",
       "  'medical',\n",
       "  'comorbidities',\n",
       "  'who',\n",
       "  'recently',\n",
       "  'gave',\n",
       "  'birth',\n",
       "  'to',\n",
       "  'a',\n",
       "  'healthy',\n",
       "  'girl',\n",
       "  'child',\n",
       "  'four',\n",
       "  'months',\n",
       "  'ago',\n",
       "  'was',\n",
       "  'brought',\n",
       "  'to',\n",
       "  'the',\n",
       "  'emergency',\n",
       "  'department',\n",
       "  'with',\n",
       "  'sudden',\n",
       "  'onset',\n",
       "  'weakness',\n",
       "  'of',\n",
       "  'both',\n",
       "  'upper',\n",
       "  'and',\n",
       "  'lower',\n",
       "  'limbs',\n",
       "  'that',\n",
       "  'started',\n",
       "  'four',\n",
       "  'days',\n",
       "  'prior',\n",
       "  'and',\n",
       "  'rapidly',\n",
       "  'progressed',\n",
       "  'to',\n",
       "  'a',\n",
       "  'state',\n",
       "  'of',\n",
       "  'quadriplegia',\n",
       "  'she',\n",
       "  'was',\n",
       "  'conscious',\n",
       "  'and',\n",
       "  'obeyed',\n",
       "  'simple',\n",
       "  'commands',\n",
       "  'with',\n",
       "  'eyes',\n",
       "  'and',\n",
       "  'mouth',\n",
       "  'however',\n",
       "  'she',\n",
       "  'had',\n",
       "  'severe',\n",
       "  'dysarthria',\n",
       "  'she',\n",
       "  'had',\n",
       "  'flaccid',\n",
       "  'hyporeflexic',\n",
       "  'pure',\n",
       "  'motor',\n",
       "  'quadriplegia',\n",
       "  'with',\n",
       "  'limbs',\n",
       "  'showing',\n",
       "  'only',\n",
       "  'a',\n",
       "  'subtle',\n",
       "  'withdrawal',\n",
       "  'flicker',\n",
       "  'to',\n",
       "  'pain',\n",
       "  'mri',\n",
       "  'of',\n",
       "  'the',\n",
       "  'brain',\n",
       "  'revealed',\n",
       "  'hyperintensity',\n",
       "  'in',\n",
       "  'the',\n",
       "  'central',\n",
       "  'pons',\n",
       "  'in',\n",
       "  'diffusion-weighted',\n",
       "  'images',\n",
       "  'figure',\n",
       "  't2-weighted',\n",
       "  'images',\n",
       "  'figure',\n",
       "  'and',\n",
       "  'fluid-attenuated',\n",
       "  'inversion',\n",
       "  'recovery',\n",
       "  'flair',\n",
       "  'images',\n",
       "  'figure',\n",
       "  'without',\n",
       "  'abnormal',\n",
       "  'contrast',\n",
       "  'enhancement',\n",
       "  'figure',\n",
       "  'consistent',\n",
       "  'with',\n",
       "  'central',\n",
       "  'pontine',\n",
       "  'myelinolysis',\n",
       "  'cpm',\n",
       "  'figure',\n",
       "  'nthe',\n",
       "  'biochemical',\n",
       "  'analysis',\n",
       "  'showed',\n",
       "  'hypernatremia',\n",
       "  'while',\n",
       "  'the',\n",
       "  'remaining',\n",
       "  'electrolytes',\n",
       "  'were',\n",
       "  'normal'],\n",
       " ['the',\n",
       "  'patient',\n",
       "  'is',\n",
       "  'a',\n",
       "  '68-year-old',\n",
       "  'retired',\n",
       "  'male',\n",
       "  'born',\n",
       "  'in',\n",
       "  'aloag',\n",
       "  'and',\n",
       "  'resident',\n",
       "  'of',\n",
       "  'tambillo',\n",
       "  'a',\n",
       "  'rural',\n",
       "  'locality',\n",
       "  'in',\n",
       "  'the',\n",
       "  'vicinity',\n",
       "  'of',\n",
       "  'the',\n",
       "  'capital',\n",
       "  'of',\n",
       "  'ecuador',\n",
       "  'quito',\n",
       "  'his',\n",
       "  'medical',\n",
       "  'history',\n",
       "  'was',\n",
       "  'significant',\n",
       "  'only',\n",
       "  'for',\n",
       "  'being',\n",
       "  'a',\n",
       "  'heavy',\n",
       "  'smoker',\n",
       "  'until',\n",
       "  '2016',\n",
       "  'with',\n",
       "  'a',\n",
       "  'calculated',\n",
       "  '20',\n",
       "  'pack-year',\n",
       "  'copious',\n",
       "  'alcohol',\n",
       "  'consumption',\n",
       "  'every',\n",
       "  '15',\n",
       "  'days',\n",
       "  'until',\n",
       "  '2010',\n",
       "  'and',\n",
       "  'a',\n",
       "  'myocardial',\n",
       "  'infarction',\n",
       "  'in',\n",
       "  '2015',\n",
       "  'successfully',\n",
       "  'treated',\n",
       "  'with',\n",
       "  'stenting',\n",
       "  'acetylsalicylic',\n",
       "  'acid',\n",
       "  'and',\n",
       "  'atorvastatin',\n",
       "  'a',\n",
       "  'medication',\n",
       "  'that',\n",
       "  'he',\n",
       "  'continues',\n",
       "  'until',\n",
       "  'this',\n",
       "  'day',\n",
       "  'nin',\n",
       "  'february',\n",
       "  '2020',\n",
       "  'he',\n",
       "  'presented',\n",
       "  'dysesthesias',\n",
       "  'in',\n",
       "  'the',\n",
       "  'right',\n",
       "  'hemithorax',\n",
       "  'associated',\n",
       "  'with',\n",
       "  'pain',\n",
       "  'and',\n",
       "  'a',\n",
       "  'mass-like',\n",
       "  'sensation',\n",
       "  'in',\n",
       "  'the',\n",
       "  'same',\n",
       "  'region',\n",
       "  'this',\n",
       "  'prompted',\n",
       "  'a',\n",
       "  'visit',\n",
       "  'to',\n",
       "  'his',\n",
       "  'local',\n",
       "  'healthcare',\n",
       "  'center',\n",
       "  'part',\n",
       "  'of',\n",
       "  'the',\n",
       "  'public',\n",
       "  'health',\n",
       "  'network',\n",
       "  'where',\n",
       "  'a',\n",
       "  'chest',\n",
       "  'ct',\n",
       "  'scan',\n",
       "  'was',\n",
       "  'ordered',\n",
       "  'in',\n",
       "  'march',\n",
       "  '2020',\n",
       "  'revealing',\n",
       "  'a',\n",
       "  'solitary',\n",
       "  'pulmonary',\n",
       "  'mass',\n",
       "  'located',\n",
       "  'in',\n",
       "  'the',\n",
       "  'right',\n",
       "  'inferior',\n",
       "  'lobule',\n",
       "  'with',\n",
       "  'an',\n",
       "  'invasion',\n",
       "  'of',\n",
       "  'both',\n",
       "  'the',\n",
       "  'pleura',\n",
       "  'and',\n",
       "  'thoracic',\n",
       "  'wall',\n",
       "  'however',\n",
       "  'due',\n",
       "  'to',\n",
       "  'the',\n",
       "  'beginning',\n",
       "  'of',\n",
       "  'the',\n",
       "  'covid-19',\n",
       "  'pandemic',\n",
       "  'in',\n",
       "  'ecuador',\n",
       "  'all',\n",
       "  'further',\n",
       "  'studies',\n",
       "  'were',\n",
       "  'suspended',\n",
       "  'for',\n",
       "  'two',\n",
       "  'to',\n",
       "  'three',\n",
       "  'months',\n",
       "  'resulting',\n",
       "  'in',\n",
       "  'a',\n",
       "  'significant',\n",
       "  'delay',\n",
       "  'of',\n",
       "  'the',\n",
       "  'biopsy',\n",
       "  'which',\n",
       "  'was',\n",
       "  'undertaken',\n",
       "  'on',\n",
       "  'may',\n",
       "  '17',\n",
       "  '2020',\n",
       "  'in',\n",
       "  'december',\n",
       "  'the',\n",
       "  'patient',\n",
       "  'presents',\n",
       "  'with',\n",
       "  'neurologic',\n",
       "  'symptoms',\n",
       "  'consisting',\n",
       "  'of',\n",
       "  'loss',\n",
       "  'of',\n",
       "  'balance',\n",
       "  'ataxic',\n",
       "  'gait',\n",
       "  'headaches',\n",
       "  'and',\n",
       "  'nausea',\n",
       "  'prompting',\n",
       "  'the',\n",
       "  'necessity',\n",
       "  'of',\n",
       "  'a',\n",
       "  'brain',\n",
       "  'mri'],\n",
       " ['a',\n",
       "  '68-year-old',\n",
       "  'female',\n",
       "  'patient',\n",
       "  'was',\n",
       "  'admitted',\n",
       "  'to',\n",
       "  'the',\n",
       "  'hospital',\n",
       "  'on',\n",
       "  'december',\n",
       "  '2',\n",
       "  '2020',\n",
       "  'due',\n",
       "  'to',\n",
       "  'being',\n",
       "  'anxious',\n",
       "  'and',\n",
       "  'easily',\n",
       "  'frightened',\n",
       "  'for',\n",
       "  '3',\n",
       "  'months',\n",
       "  'psychomotor',\n",
       "  'retardation',\n",
       "  'and',\n",
       "  'affected',\n",
       "  'by',\n",
       "  'urinary',\n",
       "  'incontinence',\n",
       "  'for',\n",
       "  'half',\n",
       "  'a',\n",
       "  'month',\n",
       "  'the',\n",
       "  'patient',\n",
       "  'had',\n",
       "  'no',\n",
       "  'mental',\n",
       "  'illness',\n",
       "  'before',\n",
       "  'and',\n",
       "  'developed',\n",
       "  'symptoms',\n",
       "  '3',\n",
       "  'months',\n",
       "  'before',\n",
       "  'admission',\n",
       "  'these',\n",
       "  'included',\n",
       "  'waking',\n",
       "  'up',\n",
       "  'early',\n",
       "  'being',\n",
       "  'nervous',\n",
       "  'and',\n",
       "  'afraid',\n",
       "  'for',\n",
       "  'no',\n",
       "  'apparent',\n",
       "  'reason',\n",
       "  'and',\n",
       "  'being',\n",
       "  'fearful',\n",
       "  'of',\n",
       "  'leaving',\n",
       "  'the',\n",
       "  'house',\n",
       "  'the',\n",
       "  'patient',\n",
       "  'was',\n",
       "  'upset',\n",
       "  'sensitive',\n",
       "  'and',\n",
       "  'cried',\n",
       "  'occasionally',\n",
       "  'hands',\n",
       "  'on',\n",
       "  'the',\n",
       "  'wall',\n",
       "  'at',\n",
       "  'home',\n",
       "  'to',\n",
       "  'prevent',\n",
       "  'falling',\n",
       "  'her',\n",
       "  'symptoms',\n",
       "  'improved',\n",
       "  'and',\n",
       "  'so',\n",
       "  'she',\n",
       "  'was',\n",
       "  'discharged',\n",
       "  'however',\n",
       "  'half',\n",
       "  'a',\n",
       "  'month',\n",
       "  'prior',\n",
       "  'to',\n",
       "  'admission',\n",
       "  'at',\n",
       "  'our',\n",
       "  'hospital',\n",
       "  'the',\n",
       "  'patient',\n",
       "  'experienced',\n",
       "  'a',\n",
       "  'relapse',\n",
       "  'characterized',\n",
       "  'by',\n",
       "  'anxiety',\n",
       "  'fear',\n",
       "  'small',\n",
       "  'steps',\n",
       "  'while',\n",
       "  'walking',\n",
       "  'reluctance',\n",
       "  'to',\n",
       "  'come',\n",
       "  'out',\n",
       "  'of',\n",
       "  'her',\n",
       "  'home',\n",
       "  'speaking',\n",
       "  'less',\n",
       "  'and',\n",
       "  'being',\n",
       "  'slow',\n",
       "  'to',\n",
       "  'respond',\n",
       "  'as',\n",
       "  'well',\n",
       "  'as',\n",
       "  'urinary',\n",
       "  'incontinence',\n",
       "  'however',\n",
       "  'her',\n",
       "  'gait',\n",
       "  'was',\n",
       "  'not',\n",
       "  'stable',\n",
       "  'and',\n",
       "  'she',\n",
       "  'took',\n",
       "  'small',\n",
       "  'steps',\n",
       "  'she',\n",
       "  'was',\n",
       "  'negative',\n",
       "  'for',\n",
       "  'pathological',\n",
       "  'signs',\n",
       "  'and',\n",
       "  'meningeal',\n",
       "  'irritation',\n",
       "  'in',\n",
       "  'a',\n",
       "  'psychological',\n",
       "  'assessment',\n",
       "  'the',\n",
       "  'patient',\n",
       "  'exhibited',\n",
       "  'clear',\n",
       "  'consciousness',\n",
       "  'disorientation',\n",
       "  'to',\n",
       "  'place',\n",
       "  'and',\n",
       "  'time',\n",
       "  'passivity',\n",
       "  'during',\n",
       "  'interactions',\n",
       "  'with',\n",
       "  'few',\n",
       "  'answers',\n",
       "  'to',\n",
       "  'questions',\n",
       "  'no',\n",
       "  'hallucinations',\n",
       "  'or',\n",
       "  'delusions',\n",
       "  'a',\n",
       "  'decline',\n",
       "  'in',\n",
       "  'memory',\n",
       "  'and',\n",
       "  'cognition',\n",
       "  'and',\n",
       "  'a',\n",
       "  'reduction',\n",
       "  'in',\n",
       "  'daily',\n",
       "  'physical',\n",
       "  'activities',\n",
       "  'and',\n",
       "  'energy',\n",
       "  'levels',\n",
       "  'she',\n",
       "  'felt',\n",
       "  'distraught',\n",
       "  'with',\n",
       "  'a',\n",
       "  'slight',\n",
       "  'tremor',\n",
       "  'in',\n",
       "  'her',\n",
       "  'hands',\n",
       "  'a',\n",
       "  'color',\n",
       "  'doppler',\n",
       "  'ultrasound',\n",
       "  'showed',\n",
       "  'diffused',\n",
       "  'thyroid',\n",
       "  'lesions',\n",
       "  'with',\n",
       "  'nodular',\n",
       "  'changes',\n",
       "  'nodules',\n",
       "  'in',\n",
       "  'the',\n",
       "  'right',\n",
       "  'lobe',\n",
       "  'of',\n",
       "  'the',\n",
       "  'thyroid',\n",
       "  'both',\n",
       "  'a',\n",
       "  'ct',\n",
       "  'examination',\n",
       "  'figure',\n",
       "  'and',\n",
       "  'an',\n",
       "  'mri',\n",
       "  'figure',\n",
       "  'of',\n",
       "  'the',\n",
       "  'head',\n",
       "  'showed',\n",
       "  'brain',\n",
       "  'atrophy',\n",
       "  'and',\n",
       "  'leukoaraiosis']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_list = []\n",
    "for i in range(len(osl_tokenized)):\n",
    "    orig_str = \"\"\n",
    "    for j in range(len(osl_tokenized[i])):\n",
    "        orig_str = orig_str + osl_tokenized[i][j] + ' '\n",
    "    big_list.append(orig_str)\n",
    "\n",
    "for i in range(len(big_list)):\n",
    "    big_list[i] = big_list[i].split()\n",
    "\n",
    "big_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "508f1768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "24-year-old\n",
      "dissatisfaction\n",
      "septorhinoplasty\n",
      "diffusion-weighted\n",
      "243\n",
      "992\n"
     ]
    }
   ],
   "source": [
    "max_train_character = 0\n",
    "c = 0\n",
    "for i in range(len(osl_tokenized)):\n",
    "    character_map_word = []\n",
    "    for j in range(len(osl_tokenized[i])):\n",
    "        character_map_character = []\n",
    "        c += 1\n",
    "        if (len(osl_tokenized[i][j]) > max_train_character):\n",
    "            print(osl_tokenized[i][j])\n",
    "            max_train_character = len(osl_tokenized[i][j])\n",
    "\n",
    "max_train_word = 0\n",
    "for i in range(len(osl_tokenized)):\n",
    "    if len(osl_tokenized[i]) > max_train_word:\n",
    "        max_train_word = len(osl_tokenized[i])\n",
    "print(max_train_word)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "036c301e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 243, 18])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_mapping = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10,\n",
    "                'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20,\n",
    "                'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25, '-': 26, '.': 27,\n",
    "                '0': 28, '1': 29, '2': 30, '3': 31, '4': 32, '5': 33, '6': 34, '7': 35, '8': 36, '9': 37, '<unknown>':38, '<pad>':39}\n",
    "import torch\n",
    "char_map_sentence = []\n",
    "for i in range(len(osl_tokenized)):\n",
    "    character_map_word = []\n",
    "    for j in range(len(osl_tokenized[i])):\n",
    "        character_map_character = []\n",
    "        for k in range(len(osl_tokenized[i][j])):\n",
    "            char = osl_tokenized[i][j][k]\n",
    "            if char in char_mapping:\n",
    "                character_map_character.append(char_mapping[char])\n",
    "            else:\n",
    "              \n",
    "                character_map_character.append(char_mapping['<unknown>'])\n",
    "        for k in range(max_train_character - len(osl_tokenized[i][j])):\n",
    "            character_map_character.append(char_mapping['<pad>'])\n",
    "\n",
    "        \n",
    "        character_map_word.append(character_map_character)\n",
    "\n",
    "    for j in range(max_train_word - len(osl_tokenized[i])):\n",
    "        character_map_word.append([39, ]*max_train_character)\n",
    "    char_map_sentence.append(character_map_word)\n",
    "char_map_sentence = torch.tensor(char_map_sentence)\n",
    "\n",
    "char_map_sentence.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "bb33e251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eshaan Mathur\\AppData\\Local\\Temp\\ipykernel_32296\\1622573706.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sentence_tensor = torch.tensor(sentence, dtype=torch.float)  # Assuming the character embeddings are float\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 36864 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32296\\1622573706.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mword_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# Pass through the character encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mword_encoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchar_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mencoded_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m# Stack the encoded words to form the encoded sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32296\\1622573706.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, char_embeddings)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# Pass through BiLSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Concatenate the last timestep hidden states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mlast_hidden_fwd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 812\u001b[1;33m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    813\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    814\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 36864 bytes."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the Character Encoder model with BiLSTM\n",
    "class CharacterEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CharacterEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.char_lstm = nn.LSTM(input_size, hidden_size // 2, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, char_embeddings):\n",
    "        # Pass through BiLSTM\n",
    "        lstm_out, _ = self.char_lstm(char_embeddings)\n",
    "        # Concatenate the last timestep hidden states\n",
    "        last_hidden_fwd = lstm_out[:, -1, :self.hidden_size // 2]\n",
    "        last_hidden_bwd = lstm_out[:, 0, self.hidden_size // 2:]\n",
    "        char_encoded = torch.cat((last_hidden_fwd, last_hidden_bwd), dim=1)\n",
    "        return char_encoded\n",
    "\n",
    "# Define parameters\n",
    "input_size = max_train_character  # Size of character embedding\n",
    "hidden_size = 64  # Size of hidden states in the BiLSTM\n",
    "\n",
    "# Initialize the character encoder model\n",
    "char_encoder = CharacterEncoder(input_size, hidden_size)\n",
    "\n",
    "# Iterate through each sentence in char_map_sentence\n",
    "encoded_sentences = []\n",
    "for sentence in char_map_sentence:\n",
    "    # Convert the sentence to a tensor\n",
    "    sentence_tensor = torch.tensor(sentence, dtype=torch.float)  # Assuming the character embeddings are float\n",
    "    # Pass the sentence tensor through the character encoder\n",
    "    encoded_words = []\n",
    "    for word_embeddings in sentence_tensor:\n",
    "        \n",
    "        \n",
    "        word_embeddings = word_embeddings.view(1, -1, input_size)\n",
    "        # Pass through the character encoder\n",
    "        word_encoded = char_encoder(word_embeddings)\n",
    "        encoded_words.append(word_encoded)\n",
    "    # Stack the encoded words to form the encoded sentence\n",
    "    encoded_sentence = torch.stack(encoded_words)\n",
    "    encoded_sentences.append(encoded_sentence)\n",
    "\n",
    "# Stack the encoded sentences to obtain the final encoded tensor\n",
    "final_encoded_tensor = torch.stack(encoded_sentences, dim=2).squeeze(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40093077",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_encoded_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d3eda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_encoded_tensor.shape\n",
    "big_list = []\n",
    "for i in range(len(osl_tokenized)):\n",
    "    orig_str = \"\"\n",
    "    for j in range(len(osl_tokenized[i])):\n",
    "        orig_str = orig_str + osl_tokenized[i][j] + ' '\n",
    "    big_list.append(orig_str)\n",
    "big_list\n",
    "big_list_test = []\n",
    "# for i in range(len(tsl_tokenized)):\n",
    "#     orig_str = \"\"\n",
    "#     for j in range(len(tsl_tokenized[i])):\n",
    "#         orig_str = orig_str + tsl_tokenized[i][j] + ' '\n",
    "#     big_list_test.append(orig_str)\n",
    "# big_list_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f9e60db",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_train = []\n",
    "for i in range(len(osl_tokenized)):\n",
    "    for j in range(len(osl_tokenized[i])):\n",
    "        if (osl_tokenized[i][j] not in vocab_train):\n",
    "            vocab_train.append(osl_tokenized[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f83f3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eshaan Mathur\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Eshaan Mathur\\AppData\\Local\\Temp\\ipykernel_3936\\2118448944.py\", line 25, in <module>\n",
      "    outputs = model(**tokens)\n",
      "  File \"C:\\Users\\Eshaan Mathur\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\Eshaan Mathur\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 1013, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"C:\\Users\\Eshaan Mathur\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\Eshaan Mathur\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 607, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"C:\\Users\\Eshaan Mathur\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\Eshaan Mathur\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 539, in forward\n",
      "    layer_output = apply_chunking_to_forward(\n",
      "  File \"C:\\Users\\Eshaan Mathur\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\pytorch_utils.py\", line 236, in apply_chunking_to_forward\n",
      "    return forward_fn(*input_tensors)\n",
      "  File \"C:\\Users\\Eshaan Mathur\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 551, in feed_forward_chunk\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"C:\\Users\\Eshaan Mathur\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\Eshaan Mathur\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 451, in forward\n",
      "    hidden_states = self.dense(hidden_states)\n",
      "  File \"C:\\Users\\Eshaan Mathur\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\Eshaan Mathur\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1543, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1501, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 745, in getmodule\n",
      "    for modname, module in sys.modules.copy().items():\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3936\\2118448944.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         )\n\u001b[1;32m-> 1013\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m   1014\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    606\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    608\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 539\u001b[1;33m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[0;32m    540\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2076\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2078\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2077\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2078\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2079\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2080\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "#FOR TRAINING -> THIS IS USED\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load BioBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "model = AutoModel.from_pretrained(\"dmis-lab/biobert-v1.1\").to(device)\n",
    "\n",
    "# List of wordsfrom\n",
    "word_list = vocab_train # Replace with your list of words\n",
    "\n",
    "# Initialize list to store embeddings\n",
    "word_embeddings = []\n",
    "\n",
    "# Iterate through each word\n",
    "for word in word_list:\n",
    "    # Tokenize word\n",
    "\n",
    "    tokens = tokenizer(word, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Pass tokens through BioBERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    \n",
    "    # Get embeddings for the first token (CLS token)\n",
    "    embedding = outputs.last_hidden_state[0][0]  # Assuming you want to use the CLS token embedding\n",
    "    \n",
    "    # Append embedding to list\n",
    "    word_embeddings.append(embedding)\n",
    "\n",
    "# Convert list of embeddings to tensor\n",
    "word_embeddings = torch.stack(word_embeddings).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d39cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "sentence_embeddings = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# LET US FIND SENTENCE EMBEDDINGS\n",
    "for i in range(len(osl_tokenized)):\n",
    "    each_word_embeddings = []\n",
    "    for j in range(len(osl_tokenized[i])):\n",
    "        # Create tensor and then move it to device\n",
    "        word_embedding_tensor = torch.tensor(word_embeddings[word_list.index(osl_tokenized[i][j])]).to(device)\n",
    "        each_word_embeddings.append(word_embedding_tensor)\n",
    "        \n",
    "    for t in range(max_train_word - len(osl_tokenized[i])):  # pad\n",
    "        each_word_embeddings.append(torch.zeros(768).to(device))\n",
    "    sentence_embeddings.append(torch.stack(each_word_embeddings))\n",
    "    \n",
    "sentence_embeddings = torch.stack(sentence_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_encoded_tensor.shape)\n",
    "final_encoded_tensor=final_encoded_tensor.squeeze(1).view(1798,806,64)\n",
    "final_encoded_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e807d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = final_encoded_tensor.device\n",
    "sentence_embeddings = sentence_embeddings.to(device)\n",
    "\n",
    "# Concatenate along the last dimension (dimension 2)\n",
    "input_to_word_encoder = torch.cat((final_encoded_tensor, sentence_embeddings), dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74022ae3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74038709",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_encoded_tensor.shape)\n",
    "print(sentence_embeddings.shape)\n",
    "print(input_to_word_encoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "469f6e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ''\n",
    "for i in range(len(big_list[2])):\n",
    "    s += big_list[2][i] + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9d61112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a 33-year-old female with no prior medical comorbidities who recently gave birth to a healthy girl child four months ago was brought to the emergency department with sudden onset weakness of both upper and lower limbs that started four days prior and rapidly progressed to a state of quadriplegia she was conscious and obeyed simple commands with eyes and mouth however she had severe dysarthria she had flaccid hyporeflexic pure motor quadriplegia with limbs showing only a subtle withdrawal flicker to pain mri of the brain revealed hyperintensity in the central pons in diffusion-weighted images figure t2-weighted images figure and fluid-attenuated inversion recovery flair images figure without abnormal contrast enhancement figure consistent with central pontine myelinolysis cpm figure nthe biochemical analysis showed hypernatremia while the remaining electrolytes were normal '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac29a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import string\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"max\")\n",
    "\n",
    "#AWS COMPREHEND MODEL\n",
    "import boto3\n",
    "import boto3\n",
    "\n",
    "client = boto3.client(\n",
    "    service_name='comprehendmedical',\n",
    "    region_name='us-east-1',\n",
    "    aws_access_key_id='AKIA4LX5DJ3T22SA3HVN',\n",
    "    aws_secret_access_key='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'\n",
    ")\n",
    "result = client.detect_entities(Text= s)\n",
    "entities = result['Entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e4c5feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Id': 29,\n",
       "  'BeginOffset': 2,\n",
       "  'EndOffset': 4,\n",
       "  'Score': 0.9999591112136841,\n",
       "  'Text': '33',\n",
       "  'Category': 'PROTECTED_HEALTH_INFORMATION',\n",
       "  'Type': 'AGE',\n",
       "  'Traits': []},\n",
       " {'Id': 10,\n",
       "  'BeginOffset': 43,\n",
       "  'EndOffset': 56,\n",
       "  'Score': 0.5070512890815735,\n",
       "  'Text': 'comorbidities',\n",
       "  'Category': 'MEDICAL_CONDITION',\n",
       "  'Type': 'DX_NAME',\n",
       "  'Traits': [{'Name': 'NEGATION', 'Score': 0.5081613063812256}]},\n",
       " {'Id': 30,\n",
       "  'BeginOffset': 140,\n",
       "  'EndOffset': 160,\n",
       "  'Score': 0.9979815483093262,\n",
       "  'Text': 'emergency department',\n",
       "  'Category': 'PROTECTED_HEALTH_INFORMATION',\n",
       "  'Type': 'ADDRESS',\n",
       "  'Traits': []},\n",
       " {'Id': 12,\n",
       "  'BeginOffset': 179,\n",
       "  'EndOffset': 187,\n",
       "  'Score': 0.9855011105537415,\n",
       "  'Text': 'weakness',\n",
       "  'Category': 'MEDICAL_CONDITION',\n",
       "  'Type': 'DX_NAME',\n",
       "  'Traits': [{'Name': 'SYMPTOM', 'Score': 0.8419345021247864}],\n",
       "  'Attributes': [{'Type': 'QUALITY',\n",
       "    'Score': 0.5773990750312805,\n",
       "    'RelationshipScore': 1.0,\n",
       "    'RelationshipType': 'QUALITY',\n",
       "    'Id': 11,\n",
       "    'BeginOffset': 166,\n",
       "    'EndOffset': 178,\n",
       "    'Text': 'sudden onset',\n",
       "    'Category': 'MEDICAL_CONDITION',\n",
       "    'Traits': []},\n",
       "   {'Type': 'DIRECTION',\n",
       "    'Score': 0.8451237678527832,\n",
       "    'RelationshipScore': 0.9999126195907593,\n",
       "    'RelationshipType': 'DIRECTION',\n",
       "    'Id': 1,\n",
       "    'BeginOffset': 196,\n",
       "    'EndOffset': 201,\n",
       "    'Text': 'upper',\n",
       "    'Category': 'ANATOMY',\n",
       "    'Traits': []},\n",
       "   {'Type': 'DIRECTION',\n",
       "    'Score': 0.7914708256721497,\n",
       "    'RelationshipScore': 0.9999713897705078,\n",
       "    'RelationshipType': 'DIRECTION',\n",
       "    'Id': 2,\n",
       "    'BeginOffset': 206,\n",
       "    'EndOffset': 211,\n",
       "    'Text': 'lower',\n",
       "    'Category': 'ANATOMY',\n",
       "    'Traits': []},\n",
       "   {'Type': 'SYSTEM_ORGAN_SITE',\n",
       "    'Score': 0.8565340042114258,\n",
       "    'RelationshipScore': 0.9966920614242554,\n",
       "    'RelationshipType': 'SYSTEM_ORGAN_SITE',\n",
       "    'Id': 3,\n",
       "    'BeginOffset': 212,\n",
       "    'EndOffset': 217,\n",
       "    'Text': 'limbs',\n",
       "    'Category': 'ANATOMY',\n",
       "    'Traits': []},\n",
       "   {'Type': 'ACUITY',\n",
       "    'Score': 0.5446163415908813,\n",
       "    'RelationshipScore': 0.9998238682746887,\n",
       "    'RelationshipType': 'ACUITY',\n",
       "    'Id': 13,\n",
       "    'BeginOffset': 231,\n",
       "    'EndOffset': 246,\n",
       "    'Text': 'four days prior',\n",
       "    'Category': 'MEDICAL_CONDITION',\n",
       "    'Traits': []}]},\n",
       " {'Id': 1,\n",
       "  'BeginOffset': 196,\n",
       "  'EndOffset': 201,\n",
       "  'Score': 0.8451237678527832,\n",
       "  'Text': 'upper',\n",
       "  'Category': 'ANATOMY',\n",
       "  'Type': 'DIRECTION',\n",
       "  'Traits': []},\n",
       " {'Id': 2,\n",
       "  'BeginOffset': 206,\n",
       "  'EndOffset': 211,\n",
       "  'Score': 0.7914708256721497,\n",
       "  'Text': 'lower',\n",
       "  'Category': 'ANATOMY',\n",
       "  'Type': 'DIRECTION',\n",
       "  'Traits': []},\n",
       " {'Id': 3,\n",
       "  'BeginOffset': 212,\n",
       "  'EndOffset': 217,\n",
       "  'Score': 0.8565340042114258,\n",
       "  'Text': 'limbs',\n",
       "  'Category': 'ANATOMY',\n",
       "  'Type': 'SYSTEM_ORGAN_SITE',\n",
       "  'Traits': []},\n",
       " {'Id': 13,\n",
       "  'BeginOffset': 231,\n",
       "  'EndOffset': 246,\n",
       "  'Score': 0.5446163415908813,\n",
       "  'Text': 'four days prior',\n",
       "  'Category': 'MEDICAL_CONDITION',\n",
       "  'Type': 'ACUITY',\n",
       "  'Traits': []},\n",
       " {'Id': 15,\n",
       "  'BeginOffset': 284,\n",
       "  'EndOffset': 296,\n",
       "  'Score': 0.6166344881057739,\n",
       "  'Text': 'quadriplegia',\n",
       "  'Category': 'MEDICAL_CONDITION',\n",
       "  'Type': 'DX_NAME',\n",
       "  'Traits': []},\n",
       " {'Id': 16,\n",
       "  'BeginOffset': 305,\n",
       "  'EndOffset': 314,\n",
       "  'Score': 0.9097864627838135,\n",
       "  'Text': 'conscious',\n",
       "  'Category': 'MEDICAL_CONDITION',\n",
       "  'Type': 'DX_NAME',\n",
       "  'Traits': [{'Name': 'SIGN', 'Score': 0.8635581135749817}]},\n",
       " {'Id': 17,\n",
       "  'BeginOffset': 319,\n",
       "  'EndOffset': 341,\n",
       "  'Score': 0.8885542154312134,\n",
       "  'Text': 'obeyed simple commands',\n",
       "  'Category': 'MEDICAL_CONDITION',\n",
       "  'Type': 'DX_NAME',\n",
       "  'Traits': [{'Name': 'SIGN', 'Score': 0.8262695670127869}]},\n",
       " {'Id': 4,\n",
       "  'BeginOffset': 347,\n",
       "  'EndOffset': 351,\n",
       "  'Score': 0.9278608560562134,\n",
       "  'Text': 'eyes',\n",
       "  'Category': 'ANATOMY',\n",
       "  'Type': 'SYSTEM_ORGAN_SITE',\n",
       "  'Traits': []},\n",
       " {'Id': 5,\n",
       "  'BeginOffset': 356,\n",
       "  'EndOffset': 361,\n",
       "  'Score': 0.9686679244041443,\n",
       "  'Text': 'mouth',\n",
       "  'Category': 'ANATOMY',\n",
       "  'Type': 'SYSTEM_ORGAN_SITE',\n",
       "  'Traits': []},\n",
       " {'Id': 19,\n",
       "  'BeginOffset': 385,\n",
       "  'EndOffset': 395,\n",
       "  'Score': 0.9887828230857849,\n",
       "  'Text': 'dysarthria',\n",
       "  'Category': 'MEDICAL_CONDITION',\n",
       "  'Type': 'DX_NAME',\n",
       "  'Traits': [{'Name': 'SIGN', 'Score': 0.6054149270057678}],\n",
       "  'Attributes': [{'Type': 'QUALITY',\n",
       "    'Score': 0.9927859902381897,\n",
       "    'RelationshipScore': 1.0,\n",
       "    'RelationshipType': 'QUALITY',\n",
       "    'Id': 18,\n",
       "    'BeginOffset': 378,\n",
       "    'EndOffset': 384,\n",
       "    'Text': 'severe',\n",
       "    'Category': 'MEDICAL_CONDITION',\n",
       "    'Traits': []},\n",
       "   {'Type': 'QUALITY',\n",
       "    'Score': 0.8367485404014587,\n",
       "    'RelationshipScore': 1.0,\n",
       "    'RelationshipType': 'QUALITY',\n",
       "    'Id': 20,\n",
       "    'BeginOffset': 404,\n",
       "    'EndOffset': 411,\n",
       "    'Text': 'flaccid',\n",
       "    'Category': 'MEDICAL_CONDITION',\n",
       "    'Traits': []}]},\n",
       " {'Id': 23,\n",
       "  'BeginOffset': 436,\n",
       "  'EndOffset': 448,\n",
       "  'Score': 0.418384850025177,\n",
       "  'Text': 'quadriplegia',\n",
       "  'Category': 'MEDICAL_CONDITION',\n",
       "  'Type': 'DX_NAME',\n",
       "  'Traits': [{'Name': 'SIGN', 'Score': 0.532173752784729}],\n",
       "  'Attributes': [{'Type': 'QUALITY',\n",
       "    'Score': 0.7065203785896301,\n",
       "    'RelationshipScore': 1.0,\n",
       "    'RelationshipType': 'QUALITY',\n",
       "    'Id': 21,\n",
       "    'BeginOffset': 412,\n",
       "    'EndOffset': 424,\n",
       "    'Text': 'hyporeflexic',\n",
       "    'Category': 'MEDICAL_CONDITION',\n",
       "    'Traits': []},\n",
       "   {'Type': 'QUALITY',\n",
       "    'Score': 0.49755823612213135,\n",
       "    'RelationshipScore': 1.0,\n",
       "    'RelationshipType': 'QUALITY',\n",
       "    'Id': 22,\n",
       "    'BeginOffset': 425,\n",
       "    'EndOffset': 435,\n",
       "    'Text': 'pure motor',\n",
       "    'Category': 'MEDICAL_CONDITION',\n",
       "    'Traits': []},\n",
       "   {'Type': 'SYSTEM_ORGAN_SITE',\n",
       "    'Score': 0.9913721084594727,\n",
       "    'RelationshipScore': 0.8793478012084961,\n",
       "    'RelationshipType': 'SYSTEM_ORGAN_SITE',\n",
       "    'Id': 6,\n",
       "    'BeginOffset': 454,\n",
       "    'EndOffset': 459,\n",
       "    'Text': 'limbs',\n",
       "    'Category': 'ANATOMY',\n",
       "    'Traits': []}]},\n",
       " {'Id': 6,\n",
       "  'BeginOffset': 454,\n",
       "  'EndOffset': 459,\n",
       "  'Score': 0.9913721084594727,\n",
       "  'Text': 'limbs',\n",
       "  'Category': 'ANATOMY',\n",
       "  'Type': 'SYSTEM_ORGAN_SITE',\n",
       "  'Traits': []},\n",
       " {'Id': 25,\n",
       "  'BeginOffset': 482,\n",
       "  'EndOffset': 508,\n",
       "  'Score': 0.41404446959495544,\n",
       "  'Text': 'withdrawal flicker to pain',\n",
       "  'Category': 'MEDICAL_CONDITION',\n",
       "  'Type': 'DX_NAME',\n",
       "  'Traits': [{'Name': 'SYMPTOM', 'Score': 0.5979633331298828}],\n",
       "  'Attributes': [{'Type': 'QUALITY',\n",
       "    'Score': 0.7245405912399292,\n",
       "    'RelationshipScore': 1.0,\n",
       "    'RelationshipType': 'QUALITY',\n",
       "    'Id': 24,\n",
       "    'BeginOffset': 475,\n",
       "    'EndOffset': 481,\n",
       "    'Text': 'subtle',\n",
       "    'Category': 'MEDICAL_CONDITION',\n",
       "    'Traits': []},\n",
       "   {'Type': 'SYSTEM_ORGAN_SITE',\n",
       "    'Score': 0.9920986890792847,\n",
       "    'RelationshipScore': 0.9486191868782043,\n",
       "    'RelationshipType': 'SYSTEM_ORGAN_SITE',\n",
       "    'Id': 7,\n",
       "    'BeginOffset': 520,\n",
       "    'EndOffset': 525,\n",
       "    'Text': 'brain',\n",
       "    'Category': 'ANATOMY',\n",
       "    'Traits': []}]},\n",
       " {'Id': 31,\n",
       "  'BeginOffset': 509,\n",
       "  'EndOffset': 512,\n",
       "  'Score': 0.8520698547363281,\n",
       "  'Text': 'mri',\n",
       "  'Category': 'TEST_TREATMENT_PROCEDURE',\n",
       "  'Type': 'TEST_NAME',\n",
       "  'Traits': []},\n",
       " {'Id': 7,\n",
       "  'BeginOffset': 520,\n",
       "  'EndOffset': 525,\n",
       "  'Score': 0.9920986890792847,\n",
       "  'Text': 'brain',\n",
       "  'Category': 'ANATOMY',\n",
       "  'Type': 'SYSTEM_ORGAN_SITE',\n",
       "  'Traits': []},\n",
       " {'Id': 26,\n",
       "  'BeginOffset': 535,\n",
       "  'EndOffset': 549,\n",
       "  'Score': 0.918590784072876,\n",
       "  'Text': 'hyperintensity',\n",
       "  'Category': 'MEDICAL_CONDITION',\n",
       "  'Type': 'DX_NAME',\n",
       "  'Traits': [{'Name': 'SIGN', 'Score': 0.9218007326126099}],\n",
       "  'Attributes': [{'Type': 'SYSTEM_ORGAN_SITE',\n",
       "    'Score': 0.9920986890792847,\n",
       "    'RelationshipScore': 0.9642858505249023,\n",
       "    'RelationshipType': 'SYSTEM_ORGAN_SITE',\n",
       "    'Id': 7,\n",
       "    'BeginOffset': 520,\n",
       "    'EndOffset': 525,\n",
       "    'Text': 'brain',\n",
       "    'Category': 'ANATOMY',\n",
       "    'Traits': []},\n",
       "   {'Type': 'SYSTEM_ORGAN_SITE',\n",
       "    'Score': 0.8839238286018372,\n",
       "    'RelationshipScore': 0.9998186230659485,\n",
       "    'RelationshipType': 'SYSTEM_ORGAN_SITE',\n",
       "    'Id': 8,\n",
       "    'BeginOffset': 557,\n",
       "    'EndOffset': 569,\n",
       "    'Text': 'central pons',\n",
       "    'Category': 'ANATOMY',\n",
       "    'Traits': []}]},\n",
       " {'Id': 8,\n",
       "  'BeginOffset': 557,\n",
       "  'EndOffset': 569,\n",
       "  'Score': 0.8839238286018372,\n",
       "  'Text': 'central pons',\n",
       "  'Category': 'ANATOMY',\n",
       "  'Type': 'SYSTEM_ORGAN_SITE',\n",
       "  'Traits': []},\n",
       " {'Id': 9,\n",
       "  'BeginOffset': 753,\n",
       "  'EndOffset': 768,\n",
       "  'Score': 0.5991672277450562,\n",
       "  'Text': 'central pontine',\n",
       "  'Category': 'ANATOMY',\n",
       "  'Type': 'SYSTEM_ORGAN_SITE',\n",
       "  'Traits': []},\n",
       " {'Id': 27,\n",
       "  'BeginOffset': 769,\n",
       "  'EndOffset': 781,\n",
       "  'Score': 0.6866695284843445,\n",
       "  'Text': 'myelinolysis',\n",
       "  'Category': 'MEDICAL_CONDITION',\n",
       "  'Type': 'DX_NAME',\n",
       "  'Traits': [{'Name': 'SIGN', 'Score': 0.5981041789054871}],\n",
       "  'Attributes': [{'Type': 'SYSTEM_ORGAN_SITE',\n",
       "    'Score': 0.5991672277450562,\n",
       "    'RelationshipScore': 0.9997480511665344,\n",
       "    'RelationshipType': 'SYSTEM_ORGAN_SITE',\n",
       "    'Id': 9,\n",
       "    'BeginOffset': 753,\n",
       "    'EndOffset': 768,\n",
       "    'Text': 'central pontine',\n",
       "    'Category': 'ANATOMY',\n",
       "    'Traits': []}]},\n",
       " {'Id': 35,\n",
       "  'BeginOffset': 798,\n",
       "  'EndOffset': 818,\n",
       "  'Score': 0.9165136814117432,\n",
       "  'Text': 'biochemical analysis',\n",
       "  'Category': 'TEST_TREATMENT_PROCEDURE',\n",
       "  'Type': 'TEST_NAME',\n",
       "  'Traits': [],\n",
       "  'Attributes': [{'Type': 'TEST_VALUE',\n",
       "    'Score': 0.7343533635139465,\n",
       "    'RelationshipScore': 0.9829912185668945,\n",
       "    'RelationshipType': 'TEST_VALUE',\n",
       "    'Id': 36,\n",
       "    'BeginOffset': 826,\n",
       "    'EndOffset': 839,\n",
       "    'Text': 'hypernatremia',\n",
       "    'Category': 'TEST_TREATMENT_PROCEDURE',\n",
       "    'Traits': []},\n",
       "   {'Type': 'TEST_VALUE',\n",
       "    'Score': 0.7564194202423096,\n",
       "    'RelationshipScore': 0.8470050096511841,\n",
       "    'RelationshipType': 'TEST_VALUE',\n",
       "    'Id': 38,\n",
       "    'BeginOffset': 878,\n",
       "    'EndOffset': 884,\n",
       "    'Text': 'normal',\n",
       "    'Category': 'TEST_TREATMENT_PROCEDURE',\n",
       "    'Traits': []}]},\n",
       " {'Id': 28,\n",
       "  'BeginOffset': 826,\n",
       "  'EndOffset': 839,\n",
       "  'Score': 0.9056264758110046,\n",
       "  'Text': 'hypernatremia',\n",
       "  'Category': 'MEDICAL_CONDITION',\n",
       "  'Type': 'DX_NAME',\n",
       "  'Traits': [{'Name': 'SIGN', 'Score': 0.8506525754928589}]},\n",
       " {'Id': 37,\n",
       "  'BeginOffset': 860,\n",
       "  'EndOffset': 872,\n",
       "  'Score': 0.7621392607688904,\n",
       "  'Text': 'electrolytes',\n",
       "  'Category': 'TEST_TREATMENT_PROCEDURE',\n",
       "  'Type': 'TEST_NAME',\n",
       "  'Traits': [],\n",
       "  'Attributes': [{'Type': 'TEST_VALUE',\n",
       "    'Score': 0.7564194202423096,\n",
       "    'RelationshipScore': 0.8824542164802551,\n",
       "    'RelationshipType': 'TEST_VALUE',\n",
       "    'Id': 38,\n",
       "    'BeginOffset': 878,\n",
       "    'EndOffset': 884,\n",
       "    'Text': 'normal',\n",
       "    'Category': 'TEST_TREATMENT_PROCEDURE',\n",
       "    'Traits': []}]}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57f77060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a 59-year-old female current smoker with 20 pack-years history with a past medical history only significant for hypertension gradually developed anorexia nausea fatigue and weight loss she initially presented to the emergency department with left flank pain and on ct scan of the abdomen was found to have diffuse osteosclerotic lesions in visualized bones there was no palpable mass or axillary adenopathy on breast examination she had multiple mammograms in the past some of which had shown suspicious architecture which was followed up with multiple breast ultrasounds that had revealed benign findings nuclear bone scan was unremarkable ct chest revealed no pulmonary lesions but there were small mediastinal submental and axillary lymphadenopathy and several subcutaneous lesions on the back one of which was excised and showed inclusion epidermal cyst nwhile the workup was ongoing the patient started to experience lower back pain associated with weakness of lower extremities numbness tingling and balance issues she developed constipation as well as urinary incontinence mri of the brain and spine redemonstrated similar bony lesions in vertebrae and also revealed abnormal leptomeningeal enhancement in the brainstem extending along the entire spinal cord figure '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29a56caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A 24-year-old healthy woman presented with difficulty breathing and dissatisfaction with her facial appearance. She had a history of childhood trauma resulting in nasal septum deviation and external nasal deformity. Four months after a successful and uneventful septorhinoplasty, she presented to the emergency department with blunt nasal trauma resulting in a septal hematoma, which was drained successfully; the patient was discharged with no adverse sequelae. \\\\nFour months later, the patient sustained nasal trauma again, this time accompanied by clear nasal discharge, raising suspicion of cerebrospinal fluid (CSF) leak. The patient was discharged after managing the nasal injury, as the CT brain showed an intact cribriform plate with no evidence of a CSF leak. Ten days later, she presented at the emergency department with dizziness and an unstable gait. She also had complaints of paresthesia for the past two months, beginning in her right hand and progressing to the right shoulder, arm and leg, associated with some difficulty in the execution of movements in the first and second finger of the right hand. Her right leg was quite stiff with difficulty in walking. On close inquiry, she gave history of pain in the right eye and double vision many months back, which had resolved spontaneously. \\\\nMagnetic resonance imaging (MRI) of the brain, cervical and thoracic spine demonstrated demyelinating lesions in the brain and cervical segment of the spinal cord (Figure ). '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a3915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40017220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contiguous_indices(s1, s2):\n",
    "    indices = []\n",
    "    s1_len = len(s1)\n",
    "    for z in range(len(s2) - s1_len + 1):\n",
    "        if s2[z:z + s1_len] == s1:\n",
    "            indices.append(z)\n",
    "    return indices[0]  #returns start index of sub list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "738bd0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contiguous_indices(s1, s2):\n",
    "    indices = []\n",
    "    s1_len = len(s1)\n",
    "    for z in range(len(s2) - s1_len + 1):\n",
    "        if s2[z:z + s1_len] == s1:\n",
    "            indices.append(z)\n",
    "    return indices\n",
    "\n",
    "import torch\n",
    "\n",
    "labelled_data = []\n",
    "\n",
    "for i in range(len(big_list)):\n",
    "    labelled_inner_data = torch.zeros(max_train_word, 1)\n",
    "    s = \" \".join(big_list[i])\n",
    "    xpipe = pipe(s) \n",
    "    \n",
    "    # Detect entities using AWS Comprehend\n",
    "    result = client.detect_entities(Text=s)\n",
    "    entities = result['Entities']\n",
    "    organ_occurrences = {}\n",
    "    for entity in entities:\n",
    "        if entity['Type'] == 'SYSTEM_ORGAN_SITE':\n",
    "            organ = entity['Text']\n",
    "            indices = find_contiguous_indices(organ.split(), big_list[i])\n",
    "            for q in range(len(indices)):\n",
    "                labelled_inner_data[indices[q]] = 5\n",
    "                \n",
    "                \n",
    "    for entity in entities:\n",
    "        if entity['Traits'] and (entity['Traits'][0]['Name'] in ['DIAGNOSIS', 'SYMPTOM', 'SIGN']):\n",
    "            if entity['Traits'][0]['Name'] in ['DIAGNOSIS', 'SIGN'] and entity['Score'] > 0.5:\n",
    "                symptom = entity['Text']\n",
    "                indices = find_contiguous_indices(symptom.split(), big_list[i])\n",
    "\n",
    "                if indices:\n",
    "\n",
    "                    x = symptom.split() #based on its length \n",
    "                   # print(indices)\n",
    "                    for u in range(len(indices)):\n",
    "                        for g in range(len(x)):\n",
    "                            labelled_inner_data[indices[u]+g] = 1 if g == 0 else 2\n",
    "            if entity['Traits'][0]['Name'] in ['SYMPTOM']:\n",
    "                symptom = entity['Text']\n",
    "                indices = find_contiguous_indices(symptom.split(), big_list[i])\n",
    "\n",
    "                if indices:\n",
    "\n",
    "                    x = symptom.split() #based on its length \n",
    "                   # print(indices)\n",
    "                    for u in range(len(indices)):\n",
    "                        for g in range(len(x)):\n",
    "                            labelled_inner_data[indices[u]+g] = 1 if g == 0 else 2\n",
    "    \n",
    "    # Flag affected organs\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Flag duration\n",
    "    duration_words = [x['word'] for x in xpipe if x['entity_group'] == 'Duration']\n",
    "    req = ['month','months','days', 'day','years','year','h', 'week','weeks']\n",
    "    for word in duration_words:\n",
    "        indices = None\n",
    "        for s in range(len(word.split())):\n",
    "            if word.split()[s] in req:\n",
    "                indices = find_contiguous_indices(word.split()[s].split(), big_list[i])\n",
    "        \n",
    "        if indices:\n",
    "            for o in range(len(indices)):\n",
    "                labelled_inner_data[indices[o]] = 3\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Flag sex\n",
    "    sex_words = [x['word'] for x in xpipe if x['entity_group'] == 'Sex']\n",
    "    for word in sex_words:\n",
    "        indices = find_contiguous_indices(word.split(), big_list[i])\n",
    "        for idx, val in enumerate(indices):\n",
    "            labelled_inner_data[val] = 4\n",
    "    \n",
    "    labelled_data.append(labelled_inner_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ccd5eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "labelled_data_padded = []\n",
    "\n",
    "# Find the maximum length of tensors in labelled_data\n",
    "max_length = max(len(tensor) for tensor in labelled_data)\n",
    "\n",
    "# Pad tensors to have the same length\n",
    "for tensor in labelled_data:\n",
    "    padded_tensor = torch.cat([tensor, torch.zeros(max_length - len(tensor), dtype=torch.float).unsqueeze(1)], dim=0)\n",
    "    labelled_data_padded.append(padded_tensor)\n",
    "\n",
    "\n",
    "labelled_data = torch.stack(labelled_data_padded, dim=0).view(-1, max_train_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ed0971b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a tensor(0.)\n",
      "24-year-old tensor(0.)\n",
      "healthy tensor(1.)\n",
      "woman tensor(4.)\n",
      "presented tensor(0.)\n",
      "with tensor(0.)\n",
      "difficulty tensor(1.)\n",
      "breathing tensor(2.)\n",
      "and tensor(0.)\n",
      "dissatisfaction tensor(1.)\n",
      "with tensor(0.)\n",
      "her tensor(0.)\n",
      "facial tensor(5.)\n",
      "appearance tensor(0.)\n",
      "she tensor(0.)\n",
      "had tensor(0.)\n",
      "a tensor(0.)\n",
      "history tensor(0.)\n",
      "of tensor(0.)\n",
      "childhood tensor(0.)\n",
      "trauma tensor(1.)\n",
      "resulting tensor(0.)\n",
      "in tensor(0.)\n",
      "nasal tensor(5.)\n",
      "septum tensor(0.)\n",
      "deviation tensor(1.)\n",
      "and tensor(0.)\n",
      "external tensor(0.)\n",
      "nasal tensor(5.)\n",
      "deformity tensor(1.)\n",
      "four tensor(0.)\n",
      "months tensor(3.)\n",
      "after tensor(0.)\n",
      "a tensor(0.)\n",
      "successful tensor(0.)\n",
      "and tensor(0.)\n",
      "uneventful tensor(0.)\n",
      "septorhinoplasty tensor(0.)\n",
      "she tensor(0.)\n",
      "presented tensor(0.)\n",
      "to tensor(0.)\n",
      "the tensor(0.)\n",
      "emergency tensor(0.)\n",
      "department tensor(0.)\n",
      "with tensor(0.)\n",
      "blunt tensor(0.)\n",
      "nasal tensor(5.)\n",
      "trauma tensor(1.)\n",
      "resulting tensor(0.)\n",
      "in tensor(0.)\n",
      "a tensor(0.)\n",
      "septal tensor(5.)\n",
      "hematoma tensor(1.)\n",
      "which tensor(0.)\n",
      "was tensor(0.)\n",
      "drained tensor(0.)\n",
      "successfully tensor(0.)\n",
      "the tensor(0.)\n",
      "patient tensor(0.)\n",
      "was tensor(0.)\n",
      "discharged tensor(0.)\n",
      "with tensor(0.)\n",
      "no tensor(0.)\n",
      "adverse tensor(0.)\n",
      "sequelae tensor(0.)\n",
      "nfour tensor(0.)\n",
      "months tensor(3.)\n",
      "later tensor(0.)\n",
      "the tensor(0.)\n",
      "patient tensor(0.)\n",
      "sustained tensor(0.)\n",
      "nasal tensor(5.)\n",
      "trauma tensor(1.)\n",
      "again tensor(0.)\n",
      "this tensor(0.)\n",
      "time tensor(0.)\n",
      "accompanied tensor(0.)\n",
      "by tensor(0.)\n",
      "clear tensor(0.)\n",
      "nasal tensor(5.)\n",
      "discharge tensor(1.)\n",
      "raising tensor(0.)\n",
      "suspicion tensor(0.)\n",
      "of tensor(0.)\n",
      "cerebrospinal tensor(5.)\n",
      "fluid tensor(0.)\n",
      "csf tensor(1.)\n",
      "leak tensor(2.)\n",
      "the tensor(0.)\n",
      "patient tensor(0.)\n",
      "was tensor(0.)\n",
      "discharged tensor(0.)\n",
      "after tensor(0.)\n",
      "managing tensor(0.)\n",
      "the tensor(0.)\n",
      "nasal tensor(5.)\n",
      "injury tensor(1.)\n",
      "as tensor(0.)\n",
      "the tensor(0.)\n",
      "ct tensor(0.)\n",
      "brain tensor(5.)\n",
      "showed tensor(0.)\n",
      "an tensor(0.)\n",
      "intact tensor(1.)\n",
      "cribriform tensor(2.)\n",
      "plate tensor(2.)\n",
      "with tensor(0.)\n",
      "no tensor(0.)\n",
      "evidence tensor(0.)\n",
      "of tensor(0.)\n",
      "a tensor(0.)\n",
      "csf tensor(1.)\n",
      "leak tensor(2.)\n",
      "ten tensor(0.)\n",
      "days tensor(0.)\n",
      "later tensor(0.)\n",
      "she tensor(0.)\n",
      "presented tensor(0.)\n",
      "at tensor(0.)\n",
      "the tensor(0.)\n",
      "emergency tensor(0.)\n",
      "department tensor(0.)\n",
      "with tensor(0.)\n",
      "dizziness tensor(1.)\n",
      "and tensor(0.)\n",
      "an tensor(0.)\n",
      "unstable tensor(1.)\n",
      "gait tensor(2.)\n",
      "she tensor(0.)\n",
      "also tensor(0.)\n",
      "had tensor(0.)\n",
      "complaints tensor(0.)\n",
      "of tensor(0.)\n",
      "paresthesia tensor(1.)\n",
      "for tensor(0.)\n",
      "the tensor(0.)\n",
      "past tensor(0.)\n",
      "two tensor(0.)\n",
      "months tensor(3.)\n",
      "beginning tensor(0.)\n",
      "in tensor(0.)\n",
      "her tensor(0.)\n",
      "right tensor(0.)\n",
      "hand tensor(5.)\n",
      "and tensor(0.)\n",
      "progressing tensor(0.)\n",
      "to tensor(0.)\n",
      "the tensor(0.)\n",
      "right tensor(0.)\n",
      "shoulder tensor(5.)\n",
      "arm tensor(5.)\n",
      "and tensor(0.)\n",
      "leg tensor(5.)\n",
      "associated tensor(0.)\n",
      "with tensor(0.)\n",
      "some tensor(0.)\n",
      "difficulty tensor(1.)\n",
      "in tensor(2.)\n",
      "the tensor(2.)\n",
      "execution tensor(2.)\n",
      "of tensor(2.)\n",
      "movements tensor(2.)\n",
      "in tensor(0.)\n",
      "the tensor(0.)\n",
      "first tensor(5.)\n",
      "and tensor(0.)\n",
      "second tensor(0.)\n",
      "finger tensor(0.)\n",
      "of tensor(0.)\n",
      "the tensor(0.)\n",
      "right tensor(0.)\n",
      "hand tensor(5.)\n",
      "her tensor(0.)\n",
      "right tensor(0.)\n",
      "leg tensor(5.)\n",
      "was tensor(0.)\n",
      "quite tensor(0.)\n",
      "stiff tensor(1.)\n",
      "with tensor(0.)\n",
      "difficulty tensor(0.)\n",
      "in tensor(0.)\n",
      "walking tensor(0.)\n",
      "on tensor(0.)\n",
      "close tensor(0.)\n",
      "inquiry tensor(0.)\n",
      "she tensor(0.)\n",
      "gave tensor(0.)\n",
      "history tensor(0.)\n",
      "of tensor(0.)\n",
      "pain tensor(1.)\n",
      "in tensor(0.)\n",
      "the tensor(0.)\n",
      "right tensor(0.)\n",
      "eye tensor(5.)\n",
      "and tensor(0.)\n",
      "double tensor(1.)\n",
      "vision tensor(2.)\n",
      "many tensor(0.)\n",
      "months tensor(3.)\n",
      "back tensor(0.)\n",
      "which tensor(0.)\n",
      "had tensor(0.)\n",
      "resolved tensor(0.)\n",
      "spontaneously tensor(0.)\n",
      "nmagnetic tensor(0.)\n",
      "resonance tensor(0.)\n",
      "imaging tensor(0.)\n",
      "mri tensor(0.)\n",
      "of tensor(0.)\n",
      "the tensor(0.)\n",
      "brain tensor(5.)\n",
      "cervical tensor(5.)\n",
      "and tensor(0.)\n",
      "thoracic tensor(5.)\n",
      "spine tensor(0.)\n",
      "demonstrated tensor(0.)\n",
      "demyelinating tensor(0.)\n",
      "lesions tensor(1.)\n",
      "in tensor(0.)\n",
      "the tensor(0.)\n",
      "brain tensor(5.)\n",
      "and tensor(0.)\n",
      "cervical tensor(5.)\n",
      "segment tensor(0.)\n",
      "of tensor(0.)\n",
      "the tensor(0.)\n",
      "spinal tensor(0.)\n",
      "cord tensor(0.)\n",
      "figure tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(big_list[0])):\n",
    "    print(big_list[0][i],labelled_data[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532c802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae1d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "padded_data = pad_sequence([torch.tensor(seq) for seq in labelled_data], batch_first=True)\n",
    "\n",
    "#TENSOR REPRESENTATION OF LABELS\n",
    "padded_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705759cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "\n",
    "# # Assuming 'labelled_data' is a list of lists containing labels for each word in the dataset\n",
    "# test_labelled_data = []\n",
    "# dataset = big_list_test\n",
    "# for i, text in enumerate(dataset):\n",
    "    \n",
    "#     dataset[i] = text\n",
    "#     list_entities = pipe(text)\n",
    "#     text_words = text.split()\n",
    "#     labels = [0] * len(text_words)\n",
    "\n",
    "#     for entity in list_entities:\n",
    "#         entity_word = entity['word']\n",
    "#         entity_group = entity['entity_group']\n",
    "\n",
    "#         # Check if the entity group is 'Sign_symptom' or 'Duration'\n",
    "#         if entity_group == 'Sign_symptom':\n",
    "#             entity_words = entity_word.split()\n",
    "#             for i in range(len(text_words)):\n",
    "#                 if text_words[i:i + len(entity_words)] == entity_words:\n",
    "#                     for j in range(i, i + len(entity_words)):\n",
    "#                         labels[j] = 1\n",
    "#         elif entity_group == 'Duration':\n",
    "#             entity_words = entity_word.split()\n",
    "#             for i in range(len(text_words)):\n",
    "#                 if text_words[i:i + len(entity_words)] == entity_words:\n",
    "#                     for j in range(i, i + len(entity_words)):\n",
    "#                         labels[j] = 3\n",
    "#         elif entity_group == 'Sex':\n",
    "#             entity_words = entity_word.split()\n",
    "#             for i in range(len(text_words)):\n",
    "#                 if text_words[i:i + len(entity_words)] == entity_words:\n",
    "#                     for j in range(i, i + len(entity_words)):\n",
    "#                         labels[j] = 4\n",
    "#         elif entity_group == 'Biological_structure':\n",
    "#             entity_words = entity_word.split()\n",
    "#             for i in range(len(text_words)):\n",
    "#                 if text_words[i:i + len(entity_words)] == entity_words:\n",
    "#                     for j in range(i, i + len(entity_words)):\n",
    "#                         labels[j] = 5\n",
    "\n",
    "#     # Convert 'labels' list to a tensor and move it to device\n",
    "#     labels_tensor = torch.tensor(labels).to(device)\n",
    "#     test_labelled_data.append(labels_tensor)\n",
    "\n",
    "# print(test_labelled_data)\n",
    "\n",
    "# import torch\n",
    "\n",
    "# test_labelled_continued_symptoms = []\n",
    "# '''\n",
    "# 0\n",
    "# 1 -> S-SYM\n",
    "# 2 -> I-SYM\n",
    "# 3 -> DURATION\n",
    "# 4 -> SEX\n",
    "# '''\n",
    "\n",
    "# for i in range(len(test_labelled_data)):\n",
    "#     test_labelled_data[i] = test_labelled_data[i].tolist()  # Convert tensors to lists for manipulation\n",
    "\n",
    "#     for j in range(len(test_labelled_data[i]) - 1):\n",
    "#         if test_labelled_data[i][j] == 1 and test_labelled_data[i][j] == test_labelled_data[i][j + 1]:\n",
    "#             test_labelled_data[i][j + 1] = 2\n",
    "#         if test_labelled_data[i][j] == 2 and test_labelled_data[i][j + 1] == 1:\n",
    "#             test_labelled_data[i][j + 1] = 2\n",
    "#         if test_labelled_data[i][j] == 5 and test_labelled_data[i][j + 1] == 1:\n",
    "#             test_labelled_data[i][j + 1] = 1  \n",
    "#         if test_labelled_data[i][j] == 1 and test_labelled_data[i][j + 1] == 5:\n",
    "#             test_labelled_data[i][j + 1] = 1\n",
    "\n",
    "#     # Convert the modified list back to a tensor and move it to device\n",
    "#     test_labelled_continued_symptoms_tensor = torch.tensor(test_labelled_data[i]).to(device)\n",
    "#     test_labelled_continued_symptoms.append(test_labelled_continued_symptoms_tensor)\n",
    "\n",
    "# print(test_labelled_continued_symptoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f47966",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# test_padded_data = pad_sequence([torch.tensor(seq) for seq in test_labelled_data], batch_first=True).to(device)\n",
    "\n",
    "# #TENSOR REPRESENTATION OF LABELS\n",
    "# test_padded_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63426d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(input_to_word_encoder.shape)\n",
    "print(padded_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15483475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        # Pass embeddings through LSTM\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        \n",
    "        return lstm_out\n",
    "\n",
    "class NERModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_labels):\n",
    "        super(NERModel, self).__init__()\n",
    "        self.lstm = LSTMModel(input_size, hidden_size, num_layers, 0.0)\n",
    "        self.linear = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_output = self.lstm(x)\n",
    "        embeddings = self.linear(lstm_output)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "input_size = 832  \n",
    "hidden_size = 32  \n",
    "num_layers = 1    \n",
    "num_labels = 6   \n",
    "num_sentences = 1600\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NERModel(input_size, hidden_size, num_layers, num_labels).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for classification tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0025)\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=4, factor=0.45, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 170\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0.0\n",
    "    for i in range(num_sentences):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "  \n",
    "        input_sequence = input_to_word_encoder[i].unsqueeze(0).detach().to(device) \n",
    "        target_sequence = padded_data[i].to(device) \n",
    "        target_sequence = target_sequence[:input_sequence.size(1)].to(device) \n",
    "        output = model(input_sequence.squeeze(0))\n",
    "        \n",
    "        loss = criterion(output.squeeze(0), target_sequence)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_loss = total_loss / num_sentences\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    # Step the scheduler based on the epoch loss\n",
    "    scheduler.step(epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_data[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_non_zero_labels = 0\n",
    "correct_non_zero_labels = 0\n",
    "correct_0 = 0\n",
    "total_0 = 0\n",
    "correct_labels_one =0\n",
    "total_labels_one =0\n",
    "correct_labels_two =0\n",
    "total_labels_two =0\n",
    "correct_labels_three =0\n",
    "total_labels_three =0\n",
    "correct_labels_four =0\n",
    "total_labels_four =0\n",
    "correct_labels_five =0\n",
    "total_labels_five =0\n",
    "total_correct =0\n",
    "total =0\n",
    "\n",
    "for i in range(1700, 1800):\n",
    "    \n",
    "    input_sequence = input_to_word_encoder[i].unsqueeze(0).to(device)\n",
    "    target_sequence = padded_data[i].to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "  \n",
    "    with torch.no_grad():  \n",
    "        output = model(input_sequence.squeeze(0))  \n",
    "\n",
    "   \n",
    "    _, predicted_labels = torch.max(output.squeeze(0), dim=1)\n",
    "\n",
    "\n",
    "    actual_labels = target_sequence.cpu().numpy()\n",
    "    \n",
    "\n",
    "    for j in range(len(predicted_labels)):\n",
    "        \n",
    "        if target_sequence[j]==predicted_labels[j]:\n",
    "                total_correct += 1\n",
    "        total += 1\n",
    "        if target_sequence[j] != 0 and predicted_labels[j] == target_sequence[j]:\n",
    "            correct_non_zero_labels+=1\n",
    "        if target_sequence[j] != 0:\n",
    "            total_non_zero_labels += 1 \n",
    "        if target_sequence[j] == 0 and predicted_labels[j] == target_sequence[j]:\n",
    "            correct_0+=1\n",
    "        if target_sequence[j] == 0 :\n",
    "            total_0+=1\n",
    "        if target_sequence[j] == 1 and predicted_labels[j] == target_sequence[j]:\n",
    "            correct_labels_one+=1\n",
    "        if target_sequence[j] == 1:\n",
    "            total_labels_one += 1 \n",
    "        if target_sequence[j] == 2 and predicted_labels[j] == target_sequence[j]:\n",
    "            correct_labels_two+=1\n",
    "        if target_sequence[j] == 2:\n",
    "            total_labels_two += 1 \n",
    "        if target_sequence[j] == 3 and predicted_labels[j] == target_sequence[j]:\n",
    "            correct_labels_three+=1\n",
    "        if target_sequence[j] == 3:\n",
    "            total_labels_three += 1 \n",
    "        if target_sequence[j] == 4 and predicted_labels[j] == target_sequence[j]:\n",
    "            correct_labels_four+=1\n",
    "        if target_sequence[j] == 4:\n",
    "            total_labels_four += 1 \n",
    "        if target_sequence[j] == 5 and predicted_labels[j] == target_sequence[j]:\n",
    "            correct_labels_five+=1\n",
    "        if target_sequence[j] == 5:\n",
    "            total_labels_five += 1 \n",
    "\n",
    "non_zero_accuracy = correct_non_zero_labels / total_non_zero_labels\n",
    "label_zero_accuracy = correct_0/total_0\n",
    "label_one_accuracy = correct_labels_one/total_labels_one\n",
    "label_two_accuracy = correct_labels_two/total_labels_two\n",
    "label_four_accuracy = correct_labels_four/total_labels_four\n",
    "label_three_accuracy = correct_labels_three/total_labels_three\n",
    "label_five_accuracy = correct_labels_five/total_labels_five\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy of label zero: \", label_zero_accuracy, \"Ratio: \", correct_0,\"/\",total_0)\n",
    "print(\"Accuracy of label one:\", label_one_accuracy,\"  Ratio: \", correct_labels_one,\"/\",total_labels_one)\n",
    "print(\"Accuracy of label two:\", label_two_accuracy, \"  Ratio: \", correct_labels_two,\"/\",total_labels_two)\n",
    "print(\"Accuracy of label three:\", label_three_accuracy, \"  Ratio: \", correct_labels_three,\"/\",total_labels_three)\n",
    "print(\"Accuracy of label four:\", label_four_accuracy, \"  Ratio: \", correct_labels_four,\"/\",total_labels_four)\n",
    "print(\"Accuracy of label five:\", label_five_accuracy, \"  Ratio: \", correct_labels_five,\"/\",total_labels_five)\n",
    "print(\"Accuracy of non-zero labels:\", non_zero_accuracy, \"  Ratio: \", correct_non_zero_labels,\"/\",total_non_zero_labels )\n",
    "print(\"NET ACCURACY: \", total_correct/total, \"  Ratio: \", total_correct,\"/\",total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2022af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b34a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
